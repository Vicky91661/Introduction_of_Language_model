{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Started With Langchain And Open AI\n",
    "\n",
    "- Get setup with LangCHain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain : Prompt templates, Models, and Output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG - Retrieval Augmented Generation\n",
    "1. Data Sources : PDF, JSON, URLs, Images => Data Ingestion Technique \n",
    "2. Data Translation : Converting Huge Data to Text Chunks\n",
    "3. Embedding : Text to vectors\n",
    "4. Store the vectors in the VectorStore Database\n",
    "\n",
    "\n",
    "### Vector Database\n",
    "1. FAISS\n",
    "2. ChromaDB\n",
    "3. AstroDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Chain\n",
    "Retrieval Chain is an interface, which is responsible for quering vector store DB.\n",
    "## Data Ingestion With Documents Loaders\n",
    "- Loading a data set from a specific source.\n",
    "- https://python.langchain.com/v0.2/docs/integrations/document_loaders/\n",
    "### Document loaders\n",
    "- DocumentLoaders load data into the standard LangChain Document format.\n",
    "- Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the .load method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader('speech.txt')\n",
    "text = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading from the PDF File\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader  =  PyPDFLoader('attension.pdf')\n",
    "doc = loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitting from Documents (Huge Text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to recursively split text by characters\n",
    "This text splitter is the recommended one for generic text. it is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\",\"\\n\",\"\",\"\"]. THis has the seffect of trying to keep all paragraphs(and then sentences, and then words) together as long as possible, as those would generically seeem to be the strongest semantically related pieces of text.\n",
    "- How the text is split: by list of characters.\n",
    "- How the chunk size is measured: by number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchian_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
    "final_document = text_spliter.split_documents(doc)\n",
    "final_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = \"\"\n",
    "with open(\"speech.txt\") as f:\n",
    "    speech = f.read()\n",
    "print(\"the type of the speech is when open() is used=>\",type(speech))\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader('speech.txt')\n",
    "text = loader.load()\n",
    "print(\"the type of the speech is when TextLoader() is used=>\",type(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "new_text =  new_text_splitter.create_documents([speech])\n",
    "print(\"the type of the speech is when open() is used=>\",type(new_text))\n",
    "new_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to split by  character- Character Text Splitter\n",
    "THis is the simplest method. This splits based on as given character sequence, which defaults to \"\\n\\n\". Chunk length is measured by number of characters.\n",
    "\n",
    "1. How the text is split : By single character separator.\n",
    "2. How the chunk size is measures:  by number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader('speech.txt')\n",
    "docs = loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_spliter = CharacterTextSplitter(separator = \"\\n\\n\", chunk_size=100,chunk_overlap=20)\n",
    "text_spliter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to split by HTML Header\n",
    "HTMLHeaderTextSplitter is a \"structure-aware\" chunker that splits text at the HTML elment level and adds metadata fo each header \"relevent\" to any given chunk. It can return chunks element by element or combine element with the same metadata, with the objectives of a keepinig related text grouped(more or less ) sementically and (b) preserving context-rich information cncoded in document structure. It can be suded with  other text splitter as part of a chuncking pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "headers_to_split_on  = [\n",
    "    (\"h1\",\"Header 1\"),\n",
    "    (\"h2\",\"Header 2\"),\n",
    "    (\"h3\",\"Header 3\")\n",
    "]\n",
    "html_string = '''<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Page Title</title>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<h1>My First Heading</h1>\n",
    "<p>My first paragraph.</p>\n",
    "\n",
    "</body>\n",
    "</html>'''\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "url = \"https://plato.stanford.edu/entries/goedel/\"\n",
    "headers_to_split_on  = [\n",
    "    (\"h1\",\"Header 1\"),\n",
    "    (\"h2\",\"Header 2\"),\n",
    "    (\"h3\",\"Header 3\"),\n",
    "    (\"h4\",\"Header 4\")\n",
    "]\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to split JSON Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
