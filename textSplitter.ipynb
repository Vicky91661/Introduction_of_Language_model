{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Started With Langchain And Open AI\n",
    "\n",
    "- Get setup with LangCHain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain : Prompt templates, Models, and Output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG - Retrieval Augmented Generation\n",
    "1. Data Sources : PDF, JSON, URLs, Images => Data Ingestion Technique \n",
    "2. Data Translation : Converting Huge Data to Text Chunks\n",
    "3. Embedding : Text to vectors\n",
    "4. Store the vectors in the VectorStore Database\n",
    "\n",
    "\n",
    "### Vector Database\n",
    "1. FAISS\n",
    "2. ChromaDB\n",
    "3. AstroDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Chain\n",
    "Retrieval Chain is an interface, which is responsible for quering vector store DB.\n",
    "## Data Ingestion With Documents Loaders\n",
    "- Loading a data set from a specific source.\n",
    "- https://python.langchain.com/v0.2/docs/integrations/document_loaders/\n",
    "### Document loaders\n",
    "- DocumentLoaders load data into the standard LangChain Document format.\n",
    "- Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the .load method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader('speech.txt')\n",
    "text = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading from the PDF File\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader  =  PyPDFLoader('attension.pdf')\n",
    "doc = loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitting from Documents (Huge Text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to recursively split text by characters\n",
    "This text splitter is the recommended one for generic text. it is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\",\"\\n\",\"\",\"\"]. THis has the seffect of trying to keep all paragraphs(and then sentences, and then words) together as long as possible, as those would generically seeem to be the strongest semantically related pieces of text.\n",
    "- How the text is split: by list of characters.\n",
    "- How the chunk size is measured: by number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchian_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
    "final_document = text_spliter.split_documents(doc)\n",
    "final_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = \"\"\n",
    "with open(\"speech.txt\") as f:\n",
    "    speech = f.read()\n",
    "print(\"the type of the speech is when open() is used=>\",type(speech))\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader('speech.txt')\n",
    "text = loader.load()\n",
    "print(\"the type of the speech is when TextLoader() is used=>\",type(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "new_text =  new_text_splitter.create_documents([speech])\n",
    "print(\"the type of the speech is when open() is used=>\",type(new_text))\n",
    "new_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to split by  character- Character Text Splitter\n",
    "THis is the simplest method. This splits based on as given character sequence, which defaults to \"\\n\\n\". Chunk length is measured by number of characters.\n",
    "\n",
    "1. How the text is split : By single character separator.\n",
    "2. How the chunk size is measures:  by number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader('speech.txt')\n",
    "docs = loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_spliter = CharacterTextSplitter(separator = \"\\n\\n\", chunk_size=100,chunk_overlap=20)\n",
    "text_spliter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to split by HTML Header\n",
    "HTMLHeaderTextSplitter is a \"structure-aware\" chunker that splits text at the HTML elment level and adds metadata fo each header \"relevent\" to any given chunk. It can return chunks element by element or combine element with the same metadata, with the objectives of a keepinig related text grouped(more or less ) sementically and (b) preserving context-rich information cncoded in document structure. It can be suded with  other text splitter as part of a chuncking pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "headers_to_split_on  = [\n",
    "    (\"h1\",\"Header 1\"),\n",
    "    (\"h2\",\"Header 2\"),\n",
    "    (\"h3\",\"Header 3\")\n",
    "]\n",
    "html_string = '''<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Page Title</title>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<h1>My First Heading</h1>\n",
    "<p>My first paragraph.</p>\n",
    "\n",
    "</body>\n",
    "</html>'''\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "url = \"https://plato.stanford.edu/entries/goedel/\"\n",
    "headers_to_split_on  = [\n",
    "    (\"h1\",\"Header 1\"),\n",
    "    (\"h2\",\"Header 2\"),\n",
    "    (\"h3\",\"Header 3\"),\n",
    "    (\"h4\",\"Header 4\")\n",
    "]\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to split JSON Data\n",
    "This json splitter splits json data while allowing control over chunk sizes. It traverses json data depth first and builds smaller json chunks.It attempts to keep nested json objects whole but will split them if needed to keep chunks between a min_chunk_size and the max_chunk_size.\n",
    "\n",
    "if the value is not a nested json, but rather a very large string will not be split. If you need a hard cap on the chunk size consider composing this with a Recursive TExt Splitter on those chunks. Thes pre-processing step to split lists, by first converting tehm to a json(dict) and then splitting.\n",
    "\n",
    " - How the next is split : json value.\n",
    " - How the chunk size is measured : by number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "data = requests.get(\"https://jsonplaceholder.typicode.com/todos\").json()\n",
    "type(data)\n",
    "json_data = {}\n",
    "for index in range(0,len(data)):\n",
    "    json_data[index]=data[index]\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "json_splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "json_chunks = json_splitter.split_json(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in json_chunks[:3]:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The splitter can also output docuemnts\n",
    "docs = json_splitter.create_documents(texts = [json_data])\n",
    "for doc in docs[:3]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Techniques\n",
    "##### Converting text into vectors\n",
    "Embeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m      3\u001b[0m load_dotenv()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # Load all the environment varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a tutorial on OPENAI embedding\"\n",
    "query_result = embedding.embed_query(text)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the start to the making of vector then storing to the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchian_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
    "final_document = text_spliter.split_documents(doc)\n",
    "final_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to the vector\n",
    "## vector Embedding and Vector StoreDB\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(final_document,embedding)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the speech text , a sentence searched in the data base\n",
    "query = \"his dream of becoming a fighter pilot\"\n",
    "retrieved_results = db.similarity_search(query)\n",
    "retrieved_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using of open source model\n",
    "### 1. OLLAMA\n",
    "- From https://ollama.com/download download Ollama to your system\n",
    "- Open the ollma in your system\n",
    "- download any model of your like \n",
    "- For Example: using this command\n",
    "-  ollama run llama3.1\n",
    "- Ollama supports embedding models, making it possible to build retrieval augmented generation (RAG) applications that combine text prompts with existing documents or other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embedding import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = (\n",
    "    OllamaEmbeddings(model=\"gemma:2b\")  #by Default it uses llama2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = embedding.embed_documents(\n",
    "    [\n",
    "        \"Alpha is the first letter of Greek Alphabet\",\n",
    "        \"Beta is the second letter of Greek alphabet\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.embed_query(\"What is the second letter of Greek alphabet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Ollama Embedding models\n",
    "#### https://ollama.com/blog/embedding-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "text = \"This is a test document.\"\n",
    "query_result = embedding.embed_query(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Techniques using HuggingFace\n",
    "### Sentence Transformers on Hugging Face\n",
    "Hugging Face sentence-transformers is a Python framework for state of the art sentenece, text and image embeddings. One of the embeddind models is used  in the HuggingFaceEmbedding class. We have also added an alias for SentenceTransformerEmbeddings for users who are more familiar with directly using that package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is atest documents\"\n",
    "query_result = embedding.embed_query(text)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of the query result\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# giving list pf text to embed\n",
    "doc_result = embedding.embed_documents([text,\"This is not a test document.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result of 1st text embedding \n",
    "doc_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result of 2nd text embedding \n",
    "doc_result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIASS\n",
    "Facebook AI Silimarity Search is a librabry for the effiecient similarity search and clustering of dense vectors . It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fi in RAM. It also contains supporting code for evaluation and parameter tun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"speech.txt\")\n",
    "documents = loader.load()\n",
    "text_spliter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 30)\n",
    "docs = text_spliter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings()\n",
    "db = FAISS.from_documents(docs,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### querying\n",
    "query = \" He narrowly missed achieving his dream of becoming a fighter pilot.\"\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a Retriever\n",
    "We can also convert the vector store into a Retriever class. This allows us to easily use it in other LangChain methods. Which largly work with retrievers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search With Score\n",
    "There are some FAISS Specific methods. One of them is similarity_search_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance. Therfore, a lower score is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_and_score = db.similarity_search_with_score(query)\n",
    "docs_and_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = embeddings.embed_query(query)\n",
    "embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_score =  db.similarity_search_by_vector(embedding_vector)\n",
    "docs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving\n",
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading\n",
    "new_db = FAISS.load_local(\"faiss_index\",embedding,allow_dangerous_deserialization=True)\n",
    "docs = new_db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma DB\n",
    "Chroma is a AI-Native open-source vectorr database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building a sample vectordb\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"speech.txt\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size = 500,chunk_overlap = 0)\n",
    "splits = text_spliter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings()\n",
    "vectordb = Chroma.from_documents(documents=splits,embedding=embeddings)\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### querying\n",
    "query = \" He narrowly missed achieving his dream of becoming a fighter pilot.\"\n",
    "docs = vectordb.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to the disk\n",
    "vectorDB = Chroma.from_documents(documents=splits,embedding=embeddings,persist_directory = \"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from the disk\n",
    "db2 = Chroma(persist_directory = \"./chroma_db\",embedding_function = embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db2.similarity_search(query)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
